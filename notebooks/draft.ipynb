{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from datetime import UTC, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from rich import print as rprint\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.progress import (\n",
    "    BarColumn,\n",
    "    Progress,\n",
    "    TaskProgressColumn,\n",
    "    TextColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "from pygments import highlight\n",
    "from pygments.formatters import TerminalFormatter\n",
    "from pygments.lexers import HtmlLexer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config (params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "session = requests.Session()\n",
    "session.headers.update(\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Accept\": \"text/html,application/json\",\n",
    "        \"Authorization\": \"Bearer your-token\",  # if needed\n",
    "    },\n",
    ")\n",
    "\n",
    "url = \"https://myanimelist.net/topanime.php\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch page: Send a request, and extract the raw HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(session: requests.Session | None, url: str) -> str:\n",
    "    \"\"\"Fetch a page using a session.\"\"\"\n",
    "    time.sleep(random.uniform(3, 5))  # noqa: S311\n",
    "    if not session:\n",
    "        session = requests.Session()\n",
    "\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "\n",
    "    msg = f\"Failed to fetch {url}: [bold red]{response.status_code}[/bold red]\"\n",
    "    raise Exception(\n",
    "        msg,  # type: ignore\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get media items: Extract the items listed in the HTML raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(kyoumas):\n",
    "# 1. Change the `raw` parameter to a enum class\n",
    "# 2. Add start/end functionality\n",
    "\n",
    "\n",
    "def get_items_list(raw: str, max_items: int = 3) -> list[tuple[str, str]]:\n",
    "    \"\"\"Extract the media items of the current page to scrap.\"\"\"\n",
    "    soup = BeautifulSoup(raw, \"html.parser\")\n",
    "    media_list = []\n",
    "\n",
    "    for i, item in enumerate(soup.select(\"a[class=hoverinfo_trigger]\")):\n",
    "        if i >= max_items:\n",
    "            break\n",
    "\n",
    "        url = item[\"href\"]\n",
    "        name = item.text.strip()\n",
    "        media_list.append((name, url))\n",
    "\n",
    "    return media_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add read pandas DataFrame functionality\n",
    "def get_item_detail(\n",
    "    raw: str,\n",
    ") -> dict[str, int | str | list[str] | None]:\n",
    "    \"\"\"Extract the detailed data for an item.\"\"\"\n",
    "    # TODO: replace with Media class and their get_stats_url method\n",
    "    stats = {}\n",
    "    soup = BeautifulSoup(raw, \"html.parser\")\n",
    "\n",
    "    for div in soup.select(\"div[class*='spaceit_pad']\"):\n",
    "        label = div.find(\"span\", class_=\"dark_text\")\n",
    "\n",
    "        if label:\n",
    "            key = label.text.strip().rstrip(\":\").lower()\n",
    "            if key == \"score\":\n",
    "                score_span = div.find(\"span\", class_=\"score-label\")\n",
    "                if score_span:\n",
    "                    value = score_span.text.strip()\n",
    "                else:\n",
    "                    continue\n",
    "            elif key == \"ranked\" or key == \"popularity\":\n",
    "                value = label.next_sibling.strip()\n",
    "                continue\n",
    "            else:\n",
    "                links = div.find_all(\"a\")\n",
    "                # Get the text after the label and clean it\n",
    "                if len(links) > 1:\n",
    "                    value = [link.text.strip() for link in links]\n",
    "                elif len(links) == 1:\n",
    "                    value = links[0].text.strip()\n",
    "                else:\n",
    "                    value = label.next_sibling.strip()\n",
    "            stats[key] = value\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "            # Print html\n",
    "            colored_html = highlight(div.prettify(), HtmlLexer(), TerminalFormatter())\n",
    "            print(colored_html)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Storage:\n",
    "    \"\"\"Handle all file operations for the scraper.\"\"\"\n",
    "\n",
    "    def __init__(self, base_path: str | Path) -> None:\n",
    "        self.base_path = Path(base_path)\n",
    "        self.raw_path = self.base_path / \"data\" / \"raw\"\n",
    "        self.scraped_path = self.base_path / \"data\" / \"scraped\"\n",
    "        self._ensure_directories()\n",
    "\n",
    "    def _ensure_directories(self) -> None:\n",
    "        \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
    "        self.raw_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_html(self, content: str, filename: str) -> Path:\n",
    "        \"\"\"Save raw HTML content to a file.\"\"\"\n",
    "        if not filename.endswith(\".html\"):\n",
    "            filename += \".html\"\n",
    "\n",
    "        file_path = self.raw_path / filename\n",
    "        with file_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        return file_path\n",
    "\n",
    "    def read_html(self, filename: str) -> str:\n",
    "        \"\"\"Read raw HTML content from a file.\"\"\"\n",
    "        file_path = self.raw_path / filename\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(file_path)\n",
    "        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    def save_csv(self, content: list[dict], filename: str) -> Path:\n",
    "        \"\"\"Export the extracted data into a csv file.\"\"\"\n",
    "        df = pd.DataFrame(content)\n",
    "        file_path = self.scraped_path / filename\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_formatter(input_string: str) -> str:\n",
    "    \"\"\"Convert string to arbitrary name convention.\"\"\"\n",
    "    # Convert the string to lowercase\n",
    "    lower_case_string = input_string.lower()\n",
    "\n",
    "    # Replace spaces and special characters with underscores\n",
    "    snake_case_string = re.sub(r\"[^a-z0-9]+\", \"_\", lower_case_string).strip(\"_\")\n",
    "\n",
    "    # Get the current date\n",
    "    current_date = datetime.now(tz=UTC).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Combine the snake_case string with the date\n",
    "    return f\"{snake_case_string}_{current_date}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms: Frieren at the Funeral, Frieren The Slayer\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Synonyms:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " Frieren at the Funeral, Frieren The Slayer\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "japanese: 葬送のフリーレン\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Japanese:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 葬送のフリーレン\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "english: Frieren: Beyond Journey's End\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  English:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " Frieren: Beyond Journey's End\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "type: TV\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Type:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"https://myanimelist.net/topanime.php?type=tv\"\u001b[39;49;00m>\n",
      "  TV\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "episodes: 28\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Episodes:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 28\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "status: Finished Airing\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Status:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " Finished Airing\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "aired: Sep 29, 2023 to Mar 22, 2024\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Aired:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " Sep 29, 2023 to Mar 22, 2024\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "premiered: Fall 2023\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Premiered:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"https://myanimelist.net/anime/season/2023/fall\"\u001b[39;49;00m>\n",
      "  Fall 2023\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "broadcast: Fridays at 23:00 (JST)\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Broadcast:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " Fridays at 23:00 (JST)\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "producers: ['Aniplex', 'Dentsu', 'Shogakukan-Shueisha Productions', 'Nippon Television Network', 'TOHO animation', 'Shogakukan']\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Producers:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/17/Aniplex\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Aniplex\"\u001b[39;49;00m>\n",
      "  Aniplex\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/53/Dentsu\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Dentsu\"\u001b[39;49;00m>\n",
      "  Dentsu\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/62/Shogakukan-Shueisha_Productions\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Shogakukan-Shueisha Productions\"\u001b[39;49;00m>\n",
      "  Shogakukan-Shueisha Productions\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/1003/Nippon_Television_Network\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Nippon Television Network\"\u001b[39;49;00m>\n",
      "  Nippon Television Network\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/1143/TOHO_animation\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"TOHO animation\"\u001b[39;49;00m>\n",
      "  TOHO animation\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/1430/Shogakukan\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Shogakukan\"\u001b[39;49;00m>\n",
      "  Shogakukan\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "licensors: Crunchyroll\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Licensors:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/1468/Crunchyroll\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Crunchyroll\"\u001b[39;49;00m>\n",
      "  Crunchyroll\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "studios: Madhouse\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Studios:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/producer/11/Madhouse\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Madhouse\"\u001b[39;49;00m>\n",
      "  Madhouse\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "source: Manga\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Source:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"https://myanimelist.net/anime/52991/Sousou_no_Frieren#related_entries\"\u001b[39;49;00m>\n",
      "  Manga\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "genres: ['Adventure', 'Drama', 'Fantasy']\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Genres:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"genre\"\u001b[39;49;00m \u001b[36mstyle\u001b[39;49;00m=\u001b[33m\"display: none\"\u001b[39;49;00m>\n",
      "  Adventure\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/genre/2/Adventure\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Adventure\"\u001b[39;49;00m>\n",
      "  Adventure\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"genre\"\u001b[39;49;00m \u001b[36mstyle\u001b[39;49;00m=\u001b[33m\"display: none\"\u001b[39;49;00m>\n",
      "  Drama\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/genre/8/Drama\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Drama\"\u001b[39;49;00m>\n",
      "  Drama\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      " ,\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"genre\"\u001b[39;49;00m \u001b[36mstyle\u001b[39;49;00m=\u001b[33m\"display: none\"\u001b[39;49;00m>\n",
      "  Fantasy\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/genre/10/Fantasy\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Fantasy\"\u001b[39;49;00m>\n",
      "  Fantasy\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "demographic: Shounen\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Demographic:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"genre\"\u001b[39;49;00m \u001b[36mstyle\u001b[39;49;00m=\u001b[33m\"display: none\"\u001b[39;49;00m>\n",
      "  Shounen\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"/anime/genre/27/Shounen\"\u001b[39;49;00m \u001b[36mtitle\u001b[39;49;00m=\u001b[33m\"Shounen\"\u001b[39;49;00m>\n",
      "  Shounen\n",
      " </\u001b[94ma\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "duration: 24 min. per ep.\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Duration:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 24 min. per ep.\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "rating: PG-13 - Teens 13 or older\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Rating:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " PG-13 - Teens 13 or older\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "score: 9.31\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad po-r js-statistics-info di-ib\"\u001b[39;49;00m \u001b[36mdata-id\u001b[39;49;00m=\u001b[33m\"info1\"\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"aggregateRating\"\u001b[39;49;00m \u001b[36mitemscope\u001b[39;49;00m=\u001b[33m\"\"\u001b[39;49;00m \u001b[36mitemtype\u001b[39;49;00m=\u001b[33m\"http://schema.org/AggregateRating\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Score:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"score-label score-9\"\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"ratingValue\"\u001b[39;49;00m>\n",
      "  9.31\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " <\u001b[94msup\u001b[39;49;00m>\n",
      "  1\n",
      " </\u001b[94msup\u001b[39;49;00m>\n",
      " (scored by\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"ratingCount\"\u001b[39;49;00m \u001b[36mstyle\u001b[39;49;00m=\u001b[33m\"display: none\"\u001b[39;49;00m>\n",
      "  578403\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 578,403 users)\n",
      " <\u001b[94mmeta\u001b[39;49;00m \u001b[36mcontent\u001b[39;49;00m=\u001b[33m\"10\"\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"bestRating\"\u001b[39;49;00m/>\n",
      " <\u001b[94mmeta\u001b[39;49;00m \u001b[36mcontent\u001b[39;49;00m=\u001b[33m\"1\"\u001b[39;49;00m \u001b[36mitemprop\u001b[39;49;00m=\u001b[33m\"worstRating\"\u001b[39;49;00m/>\n",
      " <\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"statistics-info info1\"\u001b[39;49;00m>\n",
      "  <\u001b[94msmall\u001b[39;49;00m>\n",
      "   <\u001b[94msup\u001b[39;49;00m>\n",
      "    1\n",
      "   </\u001b[94msup\u001b[39;49;00m>\n",
      "   indicates a\n",
      "   <\u001b[94ma\u001b[39;49;00m \u001b[36mhref\u001b[39;49;00m=\u001b[33m\"javascript:void(0);\"\u001b[39;49;00m \u001b[36monclick\u001b[39;49;00m=\u001b[33m\"window.open('/info.php?go=topanime','topanime','menubar=no,scrollbars=no,status=no,width=500,height=380');\"\u001b[39;49;00m>\n",
      "    weighted score\n",
      "   </\u001b[94ma\u001b[39;49;00m>\n",
      "   .\n",
      "  </\u001b[94msmall\u001b[39;49;00m>\n",
      " </\u001b[94mdiv\u001b[39;49;00m>\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "members: 999,308\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Members:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 999,308\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n",
      "favorites: 60,525\n",
      "<\u001b[94mdiv\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"spaceit_pad\"\u001b[39;49;00m>\n",
      " <\u001b[94mspan\u001b[39;49;00m \u001b[36mclass\u001b[39;49;00m=\u001b[33m\"dark_text\"\u001b[39;49;00m>\n",
      "  Favorites:\n",
      " </\u001b[94mspan\u001b[39;49;00m>\n",
      " 60,525\n",
      "</\u001b[94mdiv\u001b[39;49;00m>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demo_scrap():\n",
    "    console = Console()\n",
    "    # Initialize storage and scraper\n",
    "    storage = Storage(\"../\")\n",
    "\n",
    "    filename = \"index.html\"\n",
    "    raw_index = storage.read_html(filename=filename)\n",
    "\n",
    "    media_items: list[dict] = []\n",
    "    items_list = get_items_list(raw=raw_index)\n",
    "\n",
    "    for i, item in enumerate(items_list, 1):\n",
    "        if i >= 2:\n",
    "            break\n",
    "\n",
    "        name = item[0]\n",
    "        item_url = item[1]\n",
    "        filename = name_formatter(name) + \".html\"\n",
    "\n",
    "        raw_detail = storage.read_html(filename=filename)\n",
    "        item_detail = get_item_detail(raw=raw_detail)\n",
    "\n",
    "        # Append to final list\n",
    "        media_items.append(item_detail)\n",
    "\n",
    "demo_dict = demo_scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(fetch: bool = False) -> list[dict[str, int | str | list[str] | None]]:\n",
    "    \"\"\"Execute functions only if is the main module.\"\"\"\n",
    "    storage = Storage(\"..\")\n",
    "    console = Console()\n",
    "\n",
    "    if fetch:\n",
    "        raw_index = fetch_page(session=session, url=url)\n",
    "        storage.save_html(content=raw_index, filename=\"index.html\")\n",
    "    else:\n",
    "        raw_index = storage.read_html(filename=\"index.html\")\n",
    "\n",
    "    media_items = []\n",
    "    items_list = get_items_list(raw=raw_index, max_items=20)\n",
    "\n",
    "    # Create progress bar with custom columns\n",
    "    with Progress(\n",
    "        TextColumn(\"[bold blue]{task.description}\"),\n",
    "        BarColumn(),\n",
    "        TaskProgressColumn(),\n",
    "        TimeRemainingColumn(),\n",
    "        console=console,\n",
    "    ) as progress:\n",
    "        # Create the main task\n",
    "        main_task = progress.add_task(\n",
    "            f\"[cyan]Processing {len(items_list)} items...\",\n",
    "            total=len(items_list),\n",
    "        )\n",
    "\n",
    "        for i, item in enumerate(items_list):\n",
    "            name = item[0]\n",
    "            filename = name_formatter(name) + \".html\"\n",
    "\n",
    "            # Update task description to show current item\n",
    "            progress.update(\n",
    "                main_task,\n",
    "                description=f\"[cyan]Processing {i + 1}/{len(items_list)}: {name[:30]}...\",\n",
    "            )\n",
    "\n",
    "            if fetch:\n",
    "                raw_detail = fetch_page(session=session, url=item[1])\n",
    "                save_path = storage.save_html(content=raw_detail, filename=filename)\n",
    "                console.print(f\"[green]Saved in {save_path}\")\n",
    "            else:\n",
    "                raw_detail = storage.read_html(filename=filename)\n",
    "\n",
    "            # Show item name in panel\n",
    "            # console.print(Panel(name))\n",
    "\n",
    "            item_detail = get_item_detail(raw=raw_detail)\n",
    "\n",
    "            # Pretty print the details\n",
    "            # console.print(pprint.pformat(item_detail))\n",
    "            media_items.append(item_detail)\n",
    "\n",
    "            # Advance the progress bar\n",
    "            progress.advance(main_task)\n",
    "\n",
    "    return media_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc57be4ec5064b249aeee9dfb50bc844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
